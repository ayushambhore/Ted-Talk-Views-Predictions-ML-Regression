{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushambhore/Ted-Talk-Views-Predictions-ML-Regression/blob/master/Ted_Talk_Views_Predictions_ML_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name : Ayush Ambhore**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to build a predictive model that can accurately predict the number of views for videos uploaded on the TED website. The dataset used for this analysis consists of 4005 rows and 19 columns, with no duplicate values but with some missing values.\n",
        "\n",
        "To prepare the data for modeling, several data cleaning and preprocessing steps were performed. The missing values in the \"all_speakers\" column were dropped since it indicated that only one speaker was present. The missing values in the \"occupations\" and \"about_speakers\" columns were replaced with \"NA\" to indicate the absence of information. The single missing value in the \"recorded_date\" column was dropped, and the missing values in the \"comments\" column were replaced with zeros.\n",
        "\n",
        "Exploratory data analysis was conducted to gain insights into the dataset. Various charts and visualizations were created to explore the most popular videos, speakers, events, languages, and topics. The analysis revealed interesting findings such as the most popular video being \"Do schools kill creativity?\" and the top speaker being Alex Gendler. Alex Gendler also had the highest number of talks delivered, while Amy Cuddy had the highest average views per video. Richard Dawkins received the most comments on his videos. The most popular event was found to be TED-ed. The density plots showed that most videos had between 100 to 250 available languages, and the majority of videos had 2 to 10 topics.\n",
        "\n",
        "Feature engineering was performed to derive new features from the existing data. The format of the \"recorded_date\" and \"published_date\" columns was changed to the datetime format. Two new columns, \"video_age_day\" and \"average_daily_views,\" were created to capture the age of the video and the average daily views. The average views of the speaker were also calculated and mapped to a new column. Unnecessary columns were dropped, and outlier treatment was performed.\n",
        "\n",
        "After feature engineering and data preprocessing, the dataset was divided into target variables and feature variables. VIF (Variance Inflation Factor) and multicollinearity checks were conducted to ensure the absence of high correlation among the features.\n",
        "\n",
        "For the implementation of the predictive model, various machine learning algorithms such as linear regression, lasso, ridge, and elastic net were used. Grid search was employed for hyperparameter tuning to optimize the model performance. However, no significant improvement was observed even after hyperparameter tuning.\n",
        "\n",
        "In conclusion, this project successfully built a predictive model to estimate the number of views for videos uploaded on the TED website. The data was cleaned, explored, and preprocessed to derive meaningful insights and create new features. Various machine learning algorithms were implemented, and though hyperparameter tuning did not yield significant improvements, the model can still serve as a valuable tool for predicting video views. The project highlights the importance of data cleaning, feature engineering, and exploratory data analysis in building effective predictive models."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ayushambhore/Ted-Talk-Views-Predictions-ML-Regression"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary objective of this project is to develop a predictive model capable of estimating the number of views for videos uploaded on the TEDx website. By analyzing various factors related to the videos, we aim to create a model that can provide valuable insights and predictions regarding the popularity of TED talks.\n",
        "\n",
        "By leveraging the available data, we will explore patterns and relationships that can help us understand the key determinants of video views. We will perform data preprocessing, feature engineering, and exploratory data analysis to gain meaningful insights into the dataset."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import calendar\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud,ImageColorGenerator\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "url = 'https://raw.githubusercontent.com/ayushambhore/Ted-Talk-Views-Predictions-ML-Regression/master/data_ted_talks.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f' Row count = {df.shape[0]}\\n Column count = {df.shape[1]}')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "ZfredsJG5k0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar=False);"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data set is about Ted talk videos. The data set have 4005 rows and 19 columns. The data have no duplicate rows. The data have 1685 missing values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. talk_id : Unique ID\n",
        "2. title : Title of the video\n",
        "3. speaker_1: Main speaker in the video\n",
        "4. all_speakers : name of all speakers in the video\n",
        "5. occupations: occupations of the main speaker\n",
        "6. about_speakers: description about the speakers\n",
        "7. views : total views on the video\n",
        "8. recorder_date: the date of the recording of the video\n",
        "9. published_date : date of publish of the video\n",
        "10. event: event name\n",
        "11. native_lang : the language in which the video was recorded\n",
        "12. available_lang : available languages for the video\n",
        "13. comments : total comments on the video\n",
        "14. duration : duration of the video\n",
        "15. topics : topics related to the video\n",
        "16. related_talks : ID and the name of the related TED video\n",
        "17. url : url link of the video\n",
        "18. description : description of the video\n",
        "19. transcript: transcript of the video"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Null values**"
      ],
      "metadata": {
        "id": "MoOI7e4P6lYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw earlier all_speakers column have 4 missing values, occupations column have 522 missing values, about_speakers have 503 missing values, recorded_date have 1 missing value and comments have 655 missing values."
      ],
      "metadata": {
        "id": "1Efz6yaS6qX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. all_speakers have 4 missing values , missing value here suggests that only one speaker was there.\n",
        "So we will drop the NaN values in all_speakers column."
      ],
      "metadata": {
        "id": "JHyUQ0E36uBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the NaN rows of all_speakers column\n",
        "df = df.dropna(subset= ['all_speakers'])"
      ],
      "metadata": {
        "id": "6bLjIeul_wke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The columns- occupations and about speakers have 522 and 503 missing values respectively.\n",
        "so we will replace these NaN values with 'NA'."
      ],
      "metadata": {
        "id": "mQgxLqMo60mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing NaN values with 'NA'\n",
        "df['occupations'].fillna(str({0:'NA'}),inplace=True,axis=0)"
      ],
      "metadata": {
        "id": "_8TPB3_z65Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['about_speakers'].fillna(str({0:'NA'}),inplace=True,axis=0)"
      ],
      "metadata": {
        "id": "ZPCeBhNWCxnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. recorded_date have only 1 missing value so we will drop that row."
      ],
      "metadata": {
        "id": "0fPfdDwLC8Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the NaN row of recorded_date column\n",
        "df = df.dropna(subset= ['recorded_date'])"
      ],
      "metadata": {
        "id": "YAV-vo3yDL6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. The comment section has the most number of missing values , which can mean 2 things either no have commented on that video or the comments are disables , keeping in mind both the cases we will replace these Nan values with 0."
      ],
      "metadata": {
        "id": "xzONJvEsDS7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing NaN values with 0\n",
        "df['comments'].fillna(0, inplace = True)"
      ],
      "metadata": {
        "id": "vLZ2Us_cD2Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking our final data"
      ],
      "metadata": {
        "id": "Wcgfl3YIETHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "BCDPtoS7EXvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar=False);"
      ],
      "metadata": {
        "id": "d5KWDRpgEf9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data have no missing values , that means our data is clean and we are ready for the next steps."
      ],
      "metadata": {
        "id": "DknyNSdEEkim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1: **Barplot** on top 10 most popular TED talk videos"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sorting the dataset with respect to views\n",
        "top10_most_views = df.sort_values(['views'],ascending=False).head(10)"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting  barplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.set(font_scale=1.5)\n",
        "plt.title('top 10 most popular TED talk videos',fontsize = 20)\n",
        "\n",
        "sns.barplot(data= top10_most_views, x='views',y= 'title',\n",
        "                    palette= \"tab10\")\n",
        "\n",
        "plt.xlabel('Views in Billions');"
      ],
      "metadata": {
        "id": "sCxoSCoXIO_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts show the frequency counts of values for the different levels of a categorical or nominal variable."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the most popular video is the 'Do schools kill creativity?'"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new content creators should note that people are more interested in these 10 topics so they should make more content on this."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2:  **Barplot** on top 10 most popular speakers on TED talk videos"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping the datafram with respect to speaker_1\n",
        "top_speakers_wrt_views = df.groupby('speaker_1')\n",
        "# Taking sum of all the rows across all the columns with respect to speaker_1\n",
        "top_speakers_wrt_views= top_speakers_wrt_views.sum()\n",
        "# sorting the values according to views in descending order and taking the top 10 values\n",
        "top_speakers_wrt_views = top_speakers_wrt_views.sort_values(['views'],ascending= False).reset_index().head(10)"
      ],
      "metadata": {
        "id": "pnyGbbMULnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting barplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.set(font_scale=1.3)\n",
        "plt.title('top 10 most popular speakers on TED talk videos',fontsize = 20)\n",
        "\n",
        "sns.barplot(data= top_speakers_wrt_views, x='views',y= 'speaker_1',\n",
        "                    palette= \"tab10\")\n",
        "\n",
        "plt.xlabel('Views in Billions')\n",
        "plt.ylabel('Speaker');"
      ],
      "metadata": {
        "id": "n0Q2KreqM1c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot provides a clear visual representation of the top 10 speakers."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the top most popular speaker is Alex Gendler."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TED organization should approach these speakers more as they are bringing the most views to their videos."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : **Countplot** on top 10 speakers with highest number of talks they delivered."
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count of top 10 speakers talks they deliverd\n",
        "speaker_vs_frequency = df['speaker_1'].value_counts().head(10)\n",
        "speaker_vs_frequency"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting countplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.set(font_scale=1.3)\n",
        "plt.title('top 10 speakers with highest number of talks they delivered',fontsize = 20)\n",
        "\n",
        "sns.countplot(y='speaker_1',\n",
        "              data=df,\n",
        "              order=speaker_vs_frequency.index,\n",
        "              palette=\"rocket\")\n",
        "\n",
        "sns.color_palette(\"rocket\")\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Speakers');"
      ],
      "metadata": {
        "id": "MXofQyG4PnNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot show the counts of observations in each categorical bin using bars. A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alex Gendler has delivered the highest number of talks."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4: **Barplot** on top 10 speakers with respect to average views"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#grouping the dataframe with respect to speaker_1\n",
        "speakers_vs_avgviews= df.groupby('speaker_1',as_index=False)['views']\n",
        "# taking mean of views\n",
        "speakers_vs_avgviews= speakers_vs_avgviews.mean()\n",
        "#sorting values with respect to mean of views in descending order\n",
        "speakers_vs_avgviews = speakers_vs_avgviews.sort_values(['views'],ascending = False).head(10)"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_param1 =speakers_vs_avgviews['speaker_1']\n",
        "x_param1 =speakers_vs_avgviews['views']"
      ],
      "metadata": {
        "id": "BJwdPBZ-WCqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting barplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.set(font_scale=1.3)\n",
        "\n",
        "graph = sns.barplot(x = x_param1,\n",
        "                    y = y_param1 ,\n",
        "                    linewidth=2,\n",
        "                    color='#ff5252')\n",
        "graph.set_title(\"Top 10 speakers with respect to average views\");"
      ],
      "metadata": {
        "id": "_xxMhrjNWNXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar plots in this study visually describe the mean of a views with respect to the speaker."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amy cuddy has the highest average views among all the speakers."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TED organization should approach these speakers more as they have the highest average views."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5: **Barplot** on Top 10 speakers with highest number of comments"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grouoing dataframe wrt speaker_1\n",
        "top_speakers_wrt_comments = df.groupby('speaker_1')\n",
        "# taking sum across all the columns\n",
        "top_speakers_wrt_comments= top_speakers_wrt_comments.sum()\n",
        "# sortinng values wrt comments in descending order\n",
        "top_speakers_wrt_comments = top_speakers_wrt_comments.sort_values(['comments'],ascending= False).reset_index().head(10)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting barplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.set(font_scale=1.3)\n",
        "plt.title('Top 10 speakers with highest number of comments',fontsize = 20)\n",
        "\n",
        "sns.barplot(data= top_speakers_wrt_comments, x='comments',y= 'speaker_1',\n",
        "                    palette= \"tab10\")\n",
        "\n",
        "plt.xlabel('comments')\n",
        "plt.ylabel('speaker');"
      ],
      "metadata": {
        "id": "KvwtS4kMYqud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot provides a clear visual representation of the top 10 speakers with respect to comments."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Richard Dawkins have the most number of comments on his videos."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6: **Barplot** on popular events"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#grouping the dataframe wrt event\n",
        "top_talk_event=df.groupby(['event'],as_index=False)\n",
        "#taking sum across views column and count on talk_id\n",
        "top_talk_event= top_talk_event.agg({'views':'sum','talk_id':'count'})\n",
        "#sorting the values wrt views\n",
        "top_talk_event= top_talk_event.sort_values('views',ascending=False).reset_index()[:8]\n",
        "top_talk_event['talk_id']=top_talk_event['views']/top_talk_event['talk_id']"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting barplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.set(font_scale=1.3)\n",
        "plt.title('popularity of events',fontsize = 20)\n",
        "\n",
        "sns.barplot(data= top_talk_event, x='views',y= 'event',\n",
        "                    palette= \"tab10\")\n",
        "\n",
        "plt.xlabel('Views in 10^9');"
      ],
      "metadata": {
        "id": "bOCxrFvzsN9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts make sense for categorical or nominal data, since they are measured on a scale with specific possible values. With categorical data, the sample is often divided into groups, and the responses have a defined order."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can clearly see the most popular event is TED-ed"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7: **Density plot** on available languages"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a new column and storing how many languages are there in available lang\n",
        "df['number_of_lang'] = df['available_lang'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting densityplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.set(font_scale=1.3)\n",
        "plt.title('Density plot on available languages',fontsize = 20)\n",
        "\n",
        "sns.distplot(df['number_of_lang'])\n",
        "plt.xlabel('Number of Languages');"
      ],
      "metadata": {
        "id": "q7XIAgQvwu8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Density Plot is the continuous and smoothed version of the Histogram estimated from the data. It is estimated through Kernel Density Estimation. In this method Kernel (continuous curve) is drawn at every individual data point and then all these curves are added together to make a single smoothened density estimation."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude from the chart that more number of videos have number languages between 100 to 250."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8: **Density plot** on number of topics"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a new column and storing how many topics are there in available lang\n",
        "df['topics'] = df.apply(lambda x: eval(x['topics']), axis=1)\n",
        "df['num_of_topics'] = df.apply(lambda x: len(x['topics']), axis=1)"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting densityplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.set(font_scale=1.3)\n",
        "plt.title('Density plot on Number of Topics',fontsize = 20)\n",
        "\n",
        "sns.distplot(df['num_of_topics'])\n",
        "plt.xlabel('Number of topics');"
      ],
      "metadata": {
        "id": "IkZc6wObxPoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Density Plot is the continuous and smoothed version of the Histogram estimated from the data. It is estimated through Kernel Density Estimation. In this method Kernel (continuous curve) is drawn at every individual data point and then all these curves are added together to make a single smoothened density estimation."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that more number of videos have around 2 to 10 topics."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9: Most frequent words used in the title with **word cloud**"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a string to add the words found in titles\n",
        "text = \" \".join(topic for topic in df.title.astype(str))\n",
        "\n",
        "#funtion for generating the wordcloud\n",
        "wordcloud = WordCloud( width=1024, height=720).generate(text)\n",
        "\n",
        "plt.title('Most frequent words used in the title',fontsize = 12)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0);"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A visualisation method that displays how frequently words appear in a given body of text, by making the size of each word proportional to its frequency. All the words are then arranged in a cluster or cloud of words."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the that world and life are the most occuring words in the titles."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10: Popular Topic tags with **word cloud**"
      ],
      "metadata": {
        "id": "ot8hKKhw6Fag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a string to add the words found in topics\n",
        "text = \" \".join(topic for topic in df.topics.astype(str))\n",
        "\n",
        "#funtion for generating the wordcloud\n",
        "wordcloud = WordCloud(width=1024, height=720).generate(text)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "\n",
        "plt.title('Popular Topic tags',fontsize = 12)\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0);"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A visualisation method that displays how frequently words appear in a given body of text, by making the size of each word proportional to its frequency. All the words are then arranged in a cluster or cloud of words."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that technology,science and global issues and TED ed are the most occuring words in the topics."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11: **Barplot** on top 10 most frequent speakers occupation."
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#value counts of occupation of the speakers\n",
        "top_10_speaker_occ =df['occupations'].value_counts()[1:].head(10).reset_index()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting barplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.set(font_scale=1.3)\n",
        "plt.title(' top 10 most frequent speakers occupation',fontsize = 20)\n",
        "\n",
        "sns.barplot(x = top_10_speaker_occ['occupations'],\n",
        "            y = top_10_speaker_occ['index'],\n",
        "                    palette= \"tab10\")\n",
        "\n",
        "\n",
        "plt.ylabel('Count',fontsize = 15);"
      ],
      "metadata": {
        "id": "Egnj-qYp7VBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts make sense for categorical or nominal data, since they are measured on a scale with specific possible values. With categorical data, the sample is often divided into groups, and the responses have a defined order."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writer is the most popular occupation of the speakers."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.set(font_scale=1)\n",
        "heatmap = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n",
        "\n",
        "heatmap.set_title('Correlation Heatmap',\n",
        "                  fontdict={'fontsize':25},\n",
        "                  pad=12);"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, i used correlation heatmap."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data manipulation"
      ],
      "metadata": {
        "id": "Xo0eItIHcfQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets look at the columns of our data set"
      ],
      "metadata": {
        "id": "41iGk5gZYfM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "9JWvXuIwJjRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets drop the unecessary columns which will not have any effect on our model building."
      ],
      "metadata": {
        "id": "_b6tetN2Ywy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns from the dataframe\n",
        "df.drop( columns = ['talk_id', 'title',\n",
        "                    'about_speakers','event', 'url',\n",
        "                    'description','transcript','native_lang',\n",
        "                    'related_talks', 'available_lang', 'topics',\n",
        "                    'all_speakers', 'occupations'],\n",
        "          inplace = True)"
      ],
      "metadata": {
        "id": "Pd3d7nxiKV7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "PjiE_Qe7KwEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before going further , we should change the format of recorded_date and published_date to datetime format."
      ],
      "metadata": {
        "id": "aDVaDRluZT9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the format of recorded_date and published_date to datetime format\n",
        "df['recorded_date'] = pd.to_datetime(df['recorded_date'], format = '%Y-%m-%d')\n",
        "df['published_date'] = pd.to_datetime(df['published_date'], format = '%Y-%m-%d')"
      ],
      "metadata": {
        "id": "I0usd8DIK98r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a new column video_age_day as it will be required for further data processing."
      ],
      "metadata": {
        "id": "uqz9Bb1QaInj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column for video_age_day\n",
        "df['video_age_day'] = df['published_date'].max() + timedelta(days=1)-(pd.DatetimeIndex(df['published_date']))\n",
        "df['video_age_day'] = df['video_age_day'].dt.days"
      ],
      "metadata": {
        "id": "fG9zIYzxPHdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating one more column for the average daily views. As this will be key feature in predicting the views of the video."
      ],
      "metadata": {
        "id": "7Y0uJpe2afrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a column for average daily views\n",
        "df['avg_daily_views'] = df['views'] / df['video_age_day']"
      ],
      "metadata": {
        "id": "c2-5oL6YR8XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining the average views of the speaker of their videos and mapping it to new column."
      ],
      "metadata": {
        "id": "qR86aLiqbeOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the average views of each speaker and map it to a new column\n",
        "speaker_temp=df.groupby('speaker_1').agg({'views' : 'mean'}).sort_values(['views'],ascending=False).to_dict().values()\n",
        "speaker_temp=  list(speaker_temp)[0]\n",
        "df['speaker_1_avg_views']=df['speaker_1'].map(speaker_temp)"
      ],
      "metadata": {
        "id": "dr_2hOMDSacq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets further drop these columns as they are not much relevant for our model."
      ],
      "metadata": {
        "id": "if6jMV6vb2Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop remaining columns\n",
        "df= df.drop(columns= ['recorded_date', 'published_date', 'speaker_1','number_of_lang' ,'num_of_topics','video_age_day'])"
      ],
      "metadata": {
        "id": "bEoD3H3uOHFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "SnEMcw1AQgo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "TFRC_5y1cnQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outliers_count = ((df < Q1 - 1.5 * IQR) | (df > Q3 + 1.5 * IQR)).sum(axis=0)\n",
        "\n",
        "outliers_count\n"
      ],
      "metadata": {
        "id": "pdCQAwqhM6tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that very less collinearity is present so we should treat these ."
      ],
      "metadata": {
        "id": "NDVNZg0kc7zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle outliers using IQR method\n",
        "lower_threshold = Q1 - 1.5 * IQR\n",
        "upper_threshold = Q3 + 1.5 * IQR\n",
        "\n",
        "df = df.mask(df < lower_threshold, Q1 - 1.5 * IQR, axis=1)\n",
        "df = df.mask(df > upper_threshold, Q3 + 1.5 * IQR, axis=1)"
      ],
      "metadata": {
        "id": "XKYya8ZWVDD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "outliers_count = ((df < Q1 - 1.5 * IQR) | (df > Q3 + 1.5 * IQR)).sum(axis=0)\n",
        "\n",
        "outliers_count"
      ],
      "metadata": {
        "id": "0rR7TSz4VStc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Assigning Target variable and feature varibales."
      ],
      "metadata": {
        "id": "r_IPk_l_dmks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['views'] #dependent variable"
      ],
      "metadata": {
        "id": "gUPKMdv3R4VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns='views') #independent variables"
      ],
      "metadata": {
        "id": "koD1MJ1HSDKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "id": "UJhzUzyLTSVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Checking VIF"
      ],
      "metadata": {
        "id": "REOERUt2d7cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Variance Inflation Factor (VIF)\n",
        "Variance inflation factor measures how much the behavior (variance) of an independent variable is influenced, or inflated, by its interaction/correlation with the other independent variables. Variance inflation factors allow a quick measure of how much a variable is contributing to the standard error in the regression."
      ],
      "metadata": {
        "id": "TXZLjBeaeJ_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check VIF (Variance Inflation Factor) to detect multicollinearity\n",
        "selected_features = ['comments', 'duration', 'avg_daily_views', 'speaker_1_avg_views']\n",
        "\n",
        "df_selected = df[selected_features]\n",
        "\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = df_selected.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df_selected.values, i) for i in range(df_selected.shape[1])]\n",
        "\n",
        "print(vif)"
      ],
      "metadata": {
        "id": "x0QPIT8XTWR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are good to go as all the VIF is under 10."
      ],
      "metadata": {
        "id": "AltKzz2qeMa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Checking multicollinearity"
      ],
      "metadata": {
        "id": "n8AWESTieWwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check multicollinearity using correlation heatmap\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.set(font_scale=1)\n",
        "heatmap = sns.heatmap(X.corr(), cmap=\"YlGnBu\", annot=True)\n",
        "\n",
        "heatmap.set_title('Correlation Heatmap',\n",
        "                  fontdict={'fontsize':25},\n",
        "                  pad=12);"
      ],
      "metadata": {
        "id": "ERm9WhAtT6sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-test split"
      ],
      "metadata": {
        "id": "ru4Wztqqhc1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - Linear Regression\n",
        "reg = LinearRegression().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model score on the training data\n",
        "reg.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "pt88_iDuiA9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "YZKWLC1yiM6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics: MSE, RMSE, MAE\n",
        "MSE  = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "MAE = mean_absolute_error(y_test, y_pred)\n",
        "print(\"MAE :\" ,MAE)"
      ],
      "metadata": {
        "id": "dAhrqT4xiPbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate R-Square and Adjusted R-Square\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R2 :\" ,r2)\n",
        "ar2= 1-(1-r2_score(y_test, y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 : \",ar2)"
      ],
      "metadata": {
        "id": "sAhCr9jCitb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics\n",
        "error_metric_regression=pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE'])"
      ],
      "metadata": {
        "id": "AvIYiV_WVoAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot to visualize predicted vs actual values\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aun3U24ikdOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation & Hyperparameter Tuning\n",
        "reg_cross = LinearRegression()\n",
        "parameters = {'positive': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "reg_cross_x = GridSearchCV(reg_cross, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "reg_cross_x.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "XZlAGOwElwst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameter value and the negative mean squared error\n",
        "print(\"The best fit positive value is found out to be :\" ,reg_cross_x.best_params_)\n",
        "print(\"\\nUsing \",reg_cross_x.best_params_, \" the negative mean squared error is: \", reg_cross_x.best_score_)"
      ],
      "metadata": {
        "id": "cNe0M-eSnloz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data using the model with the best hyperparameter value\n",
        "y_pred_reg = reg_cross_x.predict(X_test)"
      ],
      "metadata": {
        "id": "6wgfI6B8o2iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics for the tuned model and Print them\n",
        "MSE  = mean_squared_error(y_test, y_pred_reg)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "MAE = mean_absolute_error(y_test, y_pred_reg)\n",
        "print(\"MAE :\" ,MAE)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred_reg)\n",
        "print(\"R2 :\" ,r2)\n",
        "ar2 = 1-(1-r2_score(y_test, y_pred_reg))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 : \",ar2)"
      ],
      "metadata": {
        "id": "DmA_btMMUjzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics for the tuned model\n",
        "error_metric_regression_x=pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE'])"
      ],
      "metadata": {
        "id": "37v4NDQCWMDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot to visualize predicted vs actual values\n",
        "plt.scatter(y_test, y_pred_reg)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wyHdtoRxpClJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Gridsearch as Grid search works by trying every possible combination of parameters you want to try in your model."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No improvement."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - Lasso Regression\n",
        "lasso  = Lasso(alpha=0.1 , max_iter= 3000)\n",
        "lasso.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model score on the training data\n",
        "lasso.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "-lnNxqSOzB6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred_lasso = lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "iQ6hulcOzKJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics: MSE, RMSE, MAE\n",
        "MSE  = mean_squared_error(y_test, y_pred_lasso)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "MAE = mean_absolute_error(y_test, y_pred_lasso)\n",
        "print(\"MAE :\" ,MAE)\n",
        "\n",
        "# Calculate R-Square and Adjusted R-Square\n",
        "r2 = r2_score(y_test, y_pred_lasso)\n",
        "print(\"R2 :\" ,r2)\n",
        "ar2 = 1-(1-r2_score(y_test, y_pred_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 : \",ar2)"
      ],
      "metadata": {
        "id": "qXIHPw6gzXRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics\n",
        "error_metric_lasso=pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE'])"
      ],
      "metadata": {
        "id": "vZ8XFmybWcVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot to visualize predicted vs actual values\n",
        "plt.scatter(y_test, y_pred_lasso)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-dRDii5az1B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation & Hyperparameter Tuning\n",
        "lasso = Lasso()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100,1000]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "lasso_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameter value and the negative mean squared error\n",
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ],
      "metadata": {
        "id": "W8RbpAGv0LQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data using the model with the best hyperparameter value\n",
        "y_pred_lasso_x = lasso_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "WqoFZ80i0RFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics for the tuned model and Print them\n",
        "MSE  = mean_squared_error(y_test, y_pred_lasso_x)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "MAE = mean_absolute_error(y_test, y_pred_lasso_x)\n",
        "print(\"MAE :\" ,MAE)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred_lasso_x)\n",
        "print(\"R2 :\" ,r2)\n",
        "ar2 = 1-(1-r2_score(y_test, y_pred_lasso_x))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 : \",ar2)"
      ],
      "metadata": {
        "id": "fKuPKii50h9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics for the tuned model\n",
        "error_metric_lasso_x=pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE'])"
      ],
      "metadata": {
        "id": "IrNGiwoTWp44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot to visualize predicted vs actual values\n",
        "plt.scatter(y_test, y_pred_lasso_x)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LNDi59Py0Zyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Gridsearch as Grid search works by trying every possible combination of parameters you want to try in your model."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No improvement"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - Ridge Regression\n",
        "ridge = Ridge(alpha=0.1)"
      ],
      "metadata": {
        "id": "qXU9fCBq1eq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "Fp24kO7H21Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model score on the training data\n",
        "ridge.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "wvFD2d1-23jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred_r = ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "Pn9APdBz258j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics: MSE, RMSE, MAE\n",
        "MSE  = mean_squared_error(y_test, y_pred_r)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "MAE = mean_absolute_error(y_test, y_pred_r)\n",
        "print(\"MAE :\" ,MAE)\n",
        "\n",
        "# Calculate R-Square and Adjusted R-Square\n",
        "r2 = r2_score(y_test, y_pred_r)\n",
        "print(\"R2 :\" ,r2)\n",
        "ar2 = 1-(1-r2_score(y_test, y_pred_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 : \",ar2)"
      ],
      "metadata": {
        "id": "8Ke8TCW928Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics\n",
        "error_metric_ridge=pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE'])"
      ],
      "metadata": {
        "id": "82jwFiVYW05m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot to visualize predicted vs actual values\n",
        "plt.scatter(y_test, y_pred_r)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ifOtovSn3TP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation & Hyperparameter Tuning\n",
        "ridge = Ridge()\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100,6000,7000,8000]}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "ridge_regressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameter value and the negative mean squared error\n",
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)"
      ],
      "metadata": {
        "id": "wd5FCCeQ3z71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data using the model with the best hyperparameter value\n",
        "y_pred_ridge = ridge_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "a1KOZy2J32-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics for the tuned model\n",
        "MSE  = mean_squared_error(y_test, y_pred_ridge)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "MAE = mean_absolute_error(y_test, y_pred_ridge)\n",
        "print(\"MAE :\" ,MAE)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred_ridge)\n",
        "print(\"R2 :\" ,r2)\n",
        "ar2 = 1-(1-r2_score(y_test, y_pred_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 : \",ar2)"
      ],
      "metadata": {
        "id": "yML4knjw35ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics for the tuned model\n",
        "error_metric_ridge_x=pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE'])"
      ],
      "metadata": {
        "id": "55ZHqxnPXATk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot to visualize predicted vs actual values for the tuned model\n",
        "plt.scatter(y_test, y_pred_ridge)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jKB2miF04XSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Gridsearch as Grid search works by trying every possible combination of parameters you want to try in your model."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4"
      ],
      "metadata": {
        "id": "EBt700Sz5osv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - ElasticNet Regression\n",
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)"
      ],
      "metadata": {
        "id": "DIx6tkD85os6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elasticnet.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "3WAP3a2l6iD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model score on the training data\n",
        "elasticnet.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "luSgwe8C6k_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred_en = elasticnet.predict(X_test)"
      ],
      "metadata": {
        "id": "l0ojdb3i62Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics: MSE, RMSE, MAE\n",
        "MSE  = mean_squared_error(y_test, y_pred_en)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "MAE = mean_absolute_error(y_test, y_pred_en)\n",
        "print(\"MAE :\" ,MAE)\n",
        "\n",
        "# Calculate R-Square and Adjusted R-Square\n",
        "r2 = r2_score(y_test, y_pred_en)\n",
        "print(\"R2 :\" ,r2)\n",
        "ar2= 1-(1-r2_score(y_test, y_pred_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 : \",ar2)"
      ],
      "metadata": {
        "id": "98ykIyNq64c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics\n",
        "error_metric_elastic=pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE'])"
      ],
      "metadata": {
        "id": "yz6QyZ9IXIxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot to visualize predicted vs actual values\n",
        "plt.scatter(y_test, y_pred_en)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qPHr1SS07Z2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "XgZgk76X5os6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "97vYaiVN5os6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Validation & Hyperparameter Tuning\n",
        "elastic = ElasticNet()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20],'l1_ratio':[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]}\n",
        "elastic_regressor = GridSearchCV(elastic, parameters, scoring='neg_mean_squared_error',cv=5)\n",
        "elastic_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "GK_1S2tZ5os6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best hyperparameter values and the negative mean squared error\n",
        "print(\"The best fit alpha value is found out to be :\" ,elastic_regressor.best_params_)\n",
        "print(\"\\nUsing \",elastic_regressor.best_params_, \" the negative mean squared error is: \", elastic_regressor.best_score_)"
      ],
      "metadata": {
        "id": "yk-yJQtB9lR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data using the model with the best hyperparameter values\n",
        "y_pred_elastic = elastic_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "EtGC_TFK9bIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics for the tuned model\n",
        "MSE  = mean_squared_error(y_test, y_pred_elastic)\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "MAE = mean_absolute_error(y_test, y_pred_elastic)\n",
        "print(\"MAE :\" ,MAE)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred_elastic)\n",
        "print(\"R2 :\" ,r2)\n",
        "ar2= 1-(1-r2_score(y_test, y_pred_elastic))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 : \",ar2)"
      ],
      "metadata": {
        "id": "vhMhVKRz-QpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics\n",
        "error_metric_elastic_x=pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE'])"
      ],
      "metadata": {
        "id": "U9hPwNlmXR9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot to visualize predicted vs actual values\n",
        "plt.scatter(y_test, y_pred_elastic)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZLMW1YWl-gtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "c-fdlzBd5os6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Gridsearch as Grid search works by trying every possible combination of parameters you want to try in your model."
      ],
      "metadata": {
        "id": "l6wHUM0a5os6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all evaluation metrics of all the models clubbed together\n",
        "Model_Summary=pd.DataFrame({'Linear Regression':error_metric_regression['Values'],\n",
        "                            'Lasso':error_metric_lasso['Values'],\n",
        "                            'Ridge':error_metric_ridge['Values'],\n",
        "                            'Elastic':error_metric_elastic['Values']},\n",
        "                           index=['R-Square','Adj. R-Square','MSE','RMSE','MAE'])\n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
        "Model_Summary.index.name = 'Metrics'\n"
      ],
      "metadata": {
        "id": "twzAqd0wXp6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Summary"
      ],
      "metadata": {
        "id": "VwK98pwcYbC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all evaluation metrics of all the models clubbed together after hyper parameter tuning\n",
        "Model_Summary_HT =pd.DataFrame({'Linear Regression':error_metric_regression_x['Values'],\n",
        "                            'Lasso':error_metric_lasso_x['Values'],\n",
        "                            'Ridge':error_metric_ridge_x['Values'],\n",
        "                            'Elastic':error_metric_elastic_x['Values']},\n",
        "                           index=['R-Square','Adj. R-Square','MSE','RMSE','MAE'])\n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '%.6f' % x)"
      ],
      "metadata": {
        "id": "nWQAV6e_5VBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Summary_HT"
      ],
      "metadata": {
        "id": "Itnrsmgs5gIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a positive business impact we should choose R2 as it produced the best results in each of the regression models. And there is no overfitting as shown by adjusted R2."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should choose the Lasso model as it have the least RMSE value among all the other models. and after hyperparameter tuning the model works better."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The project aimed to build a predictive model to estimate the views of videos uploaded on the TED website.\n",
        "* The dataset was cleaned and preprocessed, including handling missing values and converting date columns to datetime format.\n",
        "* Exploratory data analysis provided insights into popular videos, speakers, events, languages, and topics.\n",
        "* Feature engineering was performed to create new features, such as video age and average daily views.\n",
        "Machine learning algorithms including linear regression, lasso, ridge, and elastic net were implemented.\n",
        "* The R2 metric was chosen for evaluating the models, as it consistently produced the best results in each of the regression models.\n",
        "* There was no indication of overfitting as shown by the adjusted R2 values.\n",
        "* The Lasso model was selected as the best model, as it had the lowest RMSE (Root Mean Square Error) value among all the models.\n",
        "* Hyperparameter tuning further improved the performance of the Lasso model.\n",
        "* The selected model can have a positive business impact by accurately predicting the number of views for TED videos.\n",
        "* The project emphasizes the importance of data cleaning, feature engineering, and exploratory data analysis in building effective predictive models.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}